{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Intro to the Map Reduce Paradigm  \n",
    "__ `MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2018`__\n",
    "\n",
    "Welcome to Machine Learning at Scale! This first homework assignment introduces one of the core strategies in distributed processing: divide and conquer. We'll use the simplest of tasks, word counting, to illustrate the difference between a scalable and non-scalable algorithm. You will be working with the text of _Alice in Wonderland_ to put these ideas into practice using Python and Bash scripting. By the end of this week you should be able to:\n",
    "* ... __describe__ the Bias-Variance tradeoff as it applies to Machine Learning.\n",
    "* ... __explain__ why we consider word counting to be an \"Embarrassingly Parallel\" task.\n",
    "* ... __estimate__ the runtime of embarrassingly parallel tasks using \"back of the envelope\" calculations.\n",
    "* ... __implement__ a Map Reduce algorithm using the Command Line.\n",
    "* ... __set-up__ a Docker container and know why we use them for this course.\n",
    "\n",
    "You will also  become familiar (if you aren't already) with `defaultdict`, `re` and `time` in Python, linux piping and sorting, and Jupyter magic commands `%%writefile` and `%%timeit`. __Please refer to the `README` for detailed submission instructions__.\n",
    "\n",
    "__IMPORTANT:__ If you're not familiar with linux, you should read the following tutorial reagrding **piping** and **redirecting**: https://ryanstutorials.net/linuxtutorial/piping.php You will need to understand the differences to answer some of the later questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm you are running Python 3\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.1 :: Anaconda 4.4.0 (64-bit)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version\n",
    "#! apt-get install bc\n",
    "#! apt-get install man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder for any data you download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'data': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "# NOTE: the contents of this directory will be ignored by git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Introductions\n",
    "\n",
    "`The Caterpillar and Alice looked at each other for some time in silence: at last the Caterpillar took the hookah out of its mouth, and addressed her in a languid, sleepy voice. \"Who are you?\" said the Caterpillar.`   \n",
    "<div style=\"text-align: right\"> -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4 </div>\n",
    "\n",
    "\n",
    "__Tell us about yourself! Briefly describe where you live, how far along you are in MIDS, what other classes you are taking and what you want to get out of w261.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, my name is Chandra Shekar Bikkanur. I am from Dallas, Texas, USA. This is my 5th semester in MIDS. So far, I have completed w201, w203, w207, w266, w271 and w205 courses. This semester, I am only taking w261 course. I want to be familiar with the Machine Learning implementations/solutions for large(petabyte/exabyte) datasets. From 261, I want to learn how parallel processing can be utilized to process large datasets in the context of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Bias - Variance\n",
    "__In 1-2 sentences explain the bias-variance trade off. Describe what it means to \"decompose\" sources of error. How is this used in machine learning?__ Please use mathematical equation(s) to support your explanation. (Use `$` signs to take advantage of $\\LaTeX$ formatting, eg. `$f(x)$` will look like: $f(x)$). Please also cite any sources that informed your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bias:** Bias of a model is the error due to the average difference between expected prediction to the ground truth value.\n",
    "* **Variance:** Variance of a model is the error due to the variablity in the predicted value accross different models.\n",
    "* **Bias-Variance trade off:** As bias of a model is increased, that will inturn decrease the variance of the model and vice versa. We need to find the optimized bias and variance values which will lower the prediction errors to as low as irreducible error.  \n",
    "* **Decompose source of error:** The source of error for any predicted model is a combination of squared bias, variance and irreducible error. So, decomposing the source of error means to find out the individual errors due to bias, variance and irreducible error. If we have to predict $Y$ using $X$ with a linear regression model, it can be expressed as $Y = f(X) + \\epsilon$, where $\\epsilon$ is normally distributed irreducible error. If the predicted model is $f\\hat(X)$, then the prediction error can be decomposed as below:\n",
    "$$\n",
    "Err(x) = E[(Y-f\\hat(x))^2]\n",
    "$$\n",
    "$$\n",
    "Err(x) = (E[f\\hat(x)]-f(X))^2 + E[(f\\hat(x)-E[f\\hat(x)])^2] + \\sigma_{e}^2\n",
    "$$\n",
    "$$\n",
    "Err(x) = \\text{Bias}^2 + \\text{Varaiance} + \\text{Irreducible Error}\n",
    "$$\n",
    "* **Usage in Machine Learning :** In machine learning, we use the decomposition of source of error to see the variance and bias values. This is needed to figure out if the model is overfitting or underfitting the ground truth data. Decomposition of error is also the basis for regression regularization methods such as L1 (Lasso Regression) and L2 (Ridge Regression)  \n",
    "\n",
    "_Sources:_ http://scott.fortmann-roe.com/docs/BiasVariance.html, https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Tokenizing\n",
    "A number of our assignments this term will involve extracting information from text. A common preprocessing step when working with raw files is to 'tokenize' (i.e. extract words from) the text. Within the field of Natural Language Processing a lot of thought goes into what specific tokenizing makes most sense for a given task. For example, you might choose to remove punctuation or to consider punctuation symbols  'tokens' in their own right. __In this question you'll use the Python `re` module to create a tokenizer to use when you perform WordCount on the _Alice In Wonderland_ text.__\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ In the Naive Bayes algorithm (which we'll implement next week), we'll estimate the _likelihood_ of a word by counting the number of times it appears and dividing by the size of the vocabulary (total number of unique words). Using the text: *\"Alice had an adventure that took alice to wonderland\"*, give a concrete example of how two different tokenizers could cause us to get two different results on this calculation. [`HINT`: _you should not need to read up on Naive Bayes to answer this question_]  \n",
    "  \n",
    "\n",
    "* __b) short response:__ When tokenizing in this assignment we'll remove punctuation and discard numerical digits by making everything lowercase and then capturing only consecutive letters a to z. Suppose __`tokenize(x)`__ is a Python function that performs the desired tokenization. What would __`tokenize(\"By-the-bye, what became of Alice's 12 hats?!\")`__ output? Type the answer in the space provided below.\n",
    "\n",
    "\n",
    "* __c) code:__  Fill in the regular expression pattern in the cell labeled `part c` so that the subsequent call to `re.findall(RE_PATTERN, ...)` returns the tokenization described above. [`HINT`: _we've taken care of the lowercase part for you. If regex is new to you, check out the [`re`  documentation](https://docs.python.org/3/library/re.html) or [this PyMOTW tutorial](https://pymotw.com/2/re/)._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ \n",
    "* If a tokenizer treats every word as a token, then **Alice** and **alice** will be treated as two different words and the corresponding likelihood of the word would be differnt.\n",
    "* If a tokenizer converts every word to its lower case, then **Alice** will be converted to **alice** and we will have 2 occurences of word **alice** thereby changing the likelihood of the word\n",
    "* If a tokenizer converts every word to its lowercase **and** removes stop words such as 'a', 'the', 'of', etc., then the likelihood of the word would vary as the unique words would be reduced.\n",
    "\n",
    "\n",
    "> __b)__ by the bye what became of alice s hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Part C - Fill in the regular expression\n",
    "RE_PATTERN = re.compile(\"[!.?\\-',0-9;:~`@#$%^&*()_+=<>]+\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', ',', \"'\", '12', '?!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - DO NOT MODIFY THIS CELL, just run it as is to check your pattern\n",
    "words = re.findall(RE_PATTERN, \"By-the-bye, what became of Alice's 12 hats?!\".lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "` \"Please would you tell me\", said Alice, a little timidly, for she was not quite sure whether it was good manners for her to speak first, \"why your cat grins like that?\"`  \n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4  </div>\n",
    "\n",
    "For the main part of this assignment we'll be working with the free plain text version of _Alice's Adventures in Wonderland_ available from Project Gutenberg. __Use the first two cells below to download this text and preview the first few lines.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   779k      0 --:--:-- --:--:-- --:--:--  991k\n"
     ]
    }
   ],
   "source": [
    "# Download Full text \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "���Project Gutenberg���s Alice���s Adventures in Wonderland, by Lewis Carroll\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r",
      "\r\n",
      "with this eBook or online at www.gutenberg.org\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the first few lines\n",
    "!head -n 6 data/alice.txt\n",
    "# NOTE: If you are working in JupyterLab on Docker you may not see the output \n",
    "# below due to an encoding issue... in that case, use a terminal on Docker to \n",
    "# execute this head command and confirm that the file has downloaded properly, \n",
    "# this encoding issue should not affect your work on subsequent HW items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like you to develop a habit of creating small files with simulated data for use in developing, debugging and testing your code. The jupyter magic command `%%writefile` is a convenient way to do this. __Run the following cells to create a test data file for use in our word counting task.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_test.txt\r\n",
      "alice_test.txt.output\r\n"
     ]
    }
   ],
   "source": [
    "# confirm the file was created in the data directory using a grep command:\n",
    "!ls data | grep test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Word Count in Python\n",
    "\n",
    "Over the course of this term you will also become very familiar writing Python programs that read from standard input and using Linux piping commands to run these programs and save their output to file. __In this question you will write a short python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a 'baseline implementation' that we'll later compare to the parallelized version of the same task.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) code:__ Complete the Python script in the file __`wordCount.py`__. Read the docstrings carefully to be sure you understand the expected behavior of this function. Please do not code outside of the marked location.\n",
    "\n",
    "\n",
    "* __b) testing:__ Run the cell marked `part b` to call your script on the test file we created above. Confirm that your script returns the correct counts for each word by visually comparing the output to the test file. \n",
    "\n",
    "\n",
    "* __c) results:__ When you are confident in your implementation, run the cell marked `part c` to count the number of occurrences of each word in _Alice's Adventures in Wonderland_. In the same cell we'll pipe the output to file. Then use the provided `grep` commands to check your answers.\n",
    "\n",
    "\n",
    "* __d) short response:__ Suppose you decide that you'd really like  a word and its plural (e.g. 'hatter' and 'hatters' or 'person' and 'people') to be counted as the same word. After we have run the wordcount would it be more efficient to post-process your output file or discard your output file and start the analysis over with a new tokenizer? Explain your reasoning briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a-c)__ _Complete the coding portions of this question before answering 'e'._\n",
    "\n",
    "> __d)__ It would be more efficient to post-process the file to combine plural and singular words to generate the wordcount. As majority of the words' plural form consists of same starting letters (apple -> apples), we could search for the existence of the plural form of the word and add the count as desired. On the other hand, if we have to use a new tokenizer and preprocess the entire corpora, the complexity will be huge based on the corpora size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - DO YOUR WORK IN wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this \t 3\r\n",
      "is \t 2\r\n",
      "a \t 2\r\n",
      "small \t 3\r\n",
      "test \t 3\r\n",
      "file \t 3\r\n",
      "for \t 1\r\n",
      "has \t 1\r\n",
      "two \t 1\r\n",
      "lines \t 1\r\n"
     ]
    }
   ],
   "source": [
    "# part b - DO NOT MODIFY THIS CELL, just run it as is to test your script\n",
    "!python wordCount.py < data/alice_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - DO NOT MODIFY THIS CELL, just run it as is to perform the word count.\n",
    "!python wordCount.py < data/alice.txt > data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words & their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project \t 87\r\n",
      "gutenberg \t 93\r\n",
      "s \t 219\r\n",
      "alice \t 403\r\n",
      "adventures \t 12\r\n",
      "in \t 431\r\n",
      "wonderland \t 8\r\n",
      "by \t 78\r\n",
      "lewis \t 4\r\n",
      "carroll \t 4\r\n"
     ]
    }
   ],
   "source": [
    "!head data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"alice\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice \t 403\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 403\n",
    "!grep alice data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"hatter\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatter \t 56\r\n",
      "hatters \t 1\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 56\n",
    "!grep hatter data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"queen\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen \t 75\r\n",
      "queens \t 1\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 75\n",
    "!grep queen data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Unix Sorting\n",
    "Another common task in this course's assignments will be to make strategic use of sorting. To illustrate what we mean by 'strategic,' let's return to the (admitedly artificial) scenario posed in Question 4d): _we have a file of word counts but we want to find all pairs of words and plurals (eg. 'queen' and 'queens') to join their counts into a single record_. Suppose we were going to do this by hand. A logical approach would be to start with the first word and read through each subsequent word to check if they are a singular-plural pair, then repeat this process for each word in the file.    \n",
    "\n",
    "__IMPORTANT NOTE__: For this question, pretend that all plurals simply end with 's'.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response + code:__ In the worst case scenario, how many comparisons will we have to make to perform the process described above? Use $\\LaTeX$ formatting to show the calculations that led to your answer. What is the Big O complexity of this \"algorithm\"? [`HINTs:` _To answer the first part of this question you'll want to know how many words are in your_ `alice_counts.txt` _file -- in the space provided below, write a unix command to check. If you need a Big O notation refresher, here's_ a [blog post](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/), a [cheatsheet](http://bigocheatsheet.com), and a [thorough explanation](http://pages.cs.wisc.edu/~vernon/cs367/notes/3.COMPLEXITY.html).]\n",
    "\n",
    "\n",
    "* __b) short response:__ If the word count file were sorted alphabetically, how would your approach to this task change? How many comparisons would you have to make? What would the Big O complexity of this new strategy be? *(Remember that for this question, we're assuming all plural words simply end with 's')*\n",
    "\n",
    "\n",
    "* __c) short response:__ What is the Big O complexity of the fastest sorting algorithms? When we take the time to sort into account, does it make sense to alphabetize before combining singular-plural pairs? [`HINT:` *When it comes to sorting algorithms -- Wikipedia is your friend, as is the cheatsheet from part a.*]\n",
    "\n",
    "\n",
    "* __d) code:__ Write a unix command to sort your word count file alphabetically. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) the results to `data/alice_counts_A-Z.txt`. [`HINT:` if Unix sort commands are new to you, start with [this biowize blogpost](https://biowize.wordpress.com/2015/03/13/unix-sort-sorting-with-both-numeric-and-non-numeric-keys/) or [this unixschool tutorial](http://www.theunixschool.com/2012/08/linux-sort-command-examples.html)]\n",
    "\n",
    "\n",
    "* __e) code:__ A (slightly) more likely scenario is that we'd want to look at the top 10 most frequent words that appear in the book. Write a unix command to sort your word count file from highest to lowest count. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) your results to `data/alice_counts_sorted.txt`; then run the provided cell to print the top ten words. Compare your output to the expected output we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ In our wordcount file, there are 3009 words in total. If we have to compare each word with every other word to see if it is a plural form, we need to compare words in a **nested for loop** pattern; also, we could exclude comparing the word with itself which will be 3009 less comparisons.\n",
    "$$\n",
    "\\text{Total number of comparisions for 3009 words(excluding itself): } (3009*3009)-3009 = 9051072 \\text{ Comparisons}\n",
    "$$\n",
    "$$\n",
    "\\text{The Big O complexity for this comparison/algoritm is: O(n^2)}\n",
    "$$\n",
    "\n",
    "> __b)__ If we were given an alphabetically sorted wordcount file, we could do a binary search for the plural form of a given word. The complexity for one word binary-search would be $O(log(n))$; we need perform this for all words which will make the overall complexity $O(n*log(n))$\n",
    "\n",
    "> __c)__ Big O complexity of the fastest sorting algorithm is $O(n*log(n))$. If we do not sort the wordcount and perform combining singular-plural pairs, it will take $O(n^2)$ complexity; whereas if the wordcount file is sorted($O(n*log(n))$), it will take additional $O(n*log(n)$ comparisons to combine the pairs. It does not make sense to alphabetize the words, if all we want to do is combine the singular-plural pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009 data/alice_counts.txt\r\n"
     ]
    }
   ],
   "source": [
    "# part a - write a unix command to check how many records are in your word count file\n",
    "!wc -l data/alice_counts.txt # counting the number of lines in the file to check the word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part d - unix command to sort your word counts alphabetically \n",
    "!sort data/alice_counts.txt> data/alice_counts_A-Z.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t 690\r\n",
      "abide \t 2\r\n",
      "able \t 1\r\n",
      "about \t 102\r\n",
      "above \t 3\r\n",
      "absence \t 1\r\n",
      "absurd \t 2\r\n",
      "accept \t 1\r\n",
      "acceptance \t 1\r\n",
      "accepted \t 2\r\n"
     ]
    }
   ],
   "source": [
    "# part d - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_A-Z.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part e - unix command to sort your word counts from highest to lowest count\n",
    "!sort -t \"$(echo '\\t')\" -k2nr,2 data/alice_counts.txt > data/alice_counts_sorted.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t 1818\r\n",
      "and \t 940\r\n",
      "to \t 809\r\n",
      "a \t 690\r\n",
      "of \t 631\r\n",
      "it \t 610\r\n",
      "she \t 553\r\n",
      "i \t 545\r\n",
      "you \t 481\r\n",
      "said \t 462\r\n"
     ]
    }
   ],
   "source": [
    "# part e - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_sorted.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>expected output for (d):</th>\n",
    "<th>expected output for (e):</th>\n",
    "<tr><td><pre>\n",
    "a          690\n",
    "abide      2\n",
    "able       1\n",
    "about      102\n",
    "above      3\n",
    "absence    1\n",
    "absurd     2\n",
    "accept     1\n",
    "acceptance 1\n",
    "accepted   2\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t 1818\n",
    "and\t 940\n",
    "to\t 809\n",
    "a\t 690\n",
    "of\t 631\n",
    "it\t 610\n",
    "she\t 553\n",
    "i\t 545\n",
    "you\t 481\n",
    "said\t 462\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Parallel Word Count (part 1)\n",
    "What would happen if we tried to run our script on a much larger dataset? For one thing, it would take longer to run. However there is a second concern. The Python object that aggregates our counts (`defaultdict`) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available we would crash the notebook. The solution? Divide and Conquer! Instead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. __In this question you'll use a bash script to \"parallelize\" your Word Count.__\n",
    "\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) read provided code:__ The bash script `pWordCount_v1.sh` takes an input file, splits it into a specified number of 'chunks', and then applies an executable of your choice to each chunk. Read through this code and make sure you understand each line before you proceed. [`HINT:` _For now, ignore the 'student code' section -- you'll use that in part c._]\n",
    "\n",
    "\n",
    "* __b) short response:__ Below we've provided the command to use this script to apply your analysis (`wordCount.py`) to the _Alice_ text in 4 parallel processes. We'll redirect the results into a file called `alice_pCounts.txt.` Run this analysis and compare the count for the word 'alice' to your answer from Question 4. Explain what went wrong and describe what we have to add to `pWordCount_v1.sh` to fix the problem.\n",
    "\n",
    "\n",
    "* __c) code:__ We've provided a python script, `aggregateCounts_v1.py`, which reads word counts from standard input and combines any duplicates it encounters. Read through this script to be sure you understand how it is written. Then follow the instructions in `pWordCount_v1.sh` to make a one-line modification so that it accepts `aggregateCounts_v1.py` as a 4th argument and uses this script to combine the chunk-ed word counts. Run the cell below to confirm that you now get the correct results for your 'alice' count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __b)__ As we have decared $4$ parallel processes as an argument(=$m$) for pWordCount_v1.sh, the script divided the lines into 4 chunks and performed the wordcount.py on each of the 4 chunks. As the results from these parallel processes are **not reduced**, we have 4 occurances of _alice_ with the total counts equal to 403 (as we got from Q4)\n",
    "* **Fix:** we need to provide the reducer code as an argument to pWordCount_v1.sh; and also in pWordCount_v1.sh script, we need to reduce the output received from the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - make sure your scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount_v1.sh\n",
    "!chmod a+x wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' 'wordCount.py' > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice \t 111\r\n",
      "alice \t 127\r\n",
      "alice \t 122\r\n",
      "alice \t 43\r\n"
     ]
    }
   ],
   "source": [
    "# part b - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - make sure the aggregateCounts script is executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x aggregateCounts_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v1.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# part c - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SORT(1)                          User Commands                         SORT(1)\r\n",
      "\r\n",
      "N\bNA\bAM\bME\bE\r\n",
      "       sort - sort lines of text files\r\n",
      "\r\n",
      "S\bSY\bYN\bNO\bOP\bPS\bSI\bIS\bS\r\n",
      "       s\bso\bor\brt\bt [_\bO_\bP_\bT_\bI_\bO_\bN]... [_\bF_\bI_\bL_\bE]...\r\n",
      "       s\bso\bor\brt\bt [_\bO_\bP_\bT_\bI_\bO_\bN]... _\b-_\b-_\bf_\bi_\bl_\be_\bs_\b0_\b-_\bf_\br_\bo_\bm_\b=_\bF\r\n",
      "\r\n",
      "D\bDE\bES\bSC\bCR\bRI\bIP\bPT\bTI\bIO\bON\bN\r\n",
      "       Write sorted concatenation of all FILE(s) to standard output.\r\n",
      "\r\n",
      "       With no FILE, or when FILE is -, read standard input.\r\n",
      "\r\n",
      "       Mandatory  arguments  to  long  options are mandatory for short options\r\n",
      "       too.  Ordering options:\r\n",
      "\r\n",
      "       -\b-b\bb, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-l\ble\bea\bad\bdi\bin\bng\bg-\b-b\bbl\bla\ban\bnk\bks\bs\r\n",
      "              ignore leading blanks\r\n",
      "\r\n",
      "       -\b-d\bd, -\b--\b-d\bdi\bic\bct\bti\bio\bon\bna\bar\bry\by-\b-o\bor\brd\bde\ber\br\r\n",
      "              consider only blanks and alphanumeric characters\r\n",
      "\r\n",
      "       -\b-f\bf, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-c\bca\bas\bse\be\r\n",
      "              fold lower case to upper case characters\r\n",
      "\r\n",
      "       -\b-g\bg, -\b--\b-g\bge\ben\bne\ber\bra\bal\bl-\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt\r\n",
      "              compare according to general numerical value\r\n",
      "\r\n",
      "       -\b-i\bi, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-n\bno\bon\bnp\bpr\bri\bin\bnt\bti\bin\bng\bg\r\n",
      "              consider only printable characters\r\n",
      "\r\n",
      "       -\b-M\bM, -\b--\b-m\bmo\bon\bnt\bth\bh-\b-s\bso\bor\brt\bt\r\n",
      "              compare (unknown) < 'JAN' < ... < 'DEC'\r\n",
      "\r\n",
      "       -\b-h\bh, -\b--\b-h\bhu\bum\bma\ban\bn-\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt\r\n",
      "              compare human readable numbers (e.g., 2K 1G)\r\n",
      "\r\n",
      "       -\b-n\bn, -\b--\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt\r\n",
      "              compare according to string numerical value\r\n",
      "\r\n",
      "       -\b-R\bR, -\b--\b-r\bra\ban\bnd\bdo\bom\bm-\b-s\bso\bor\brt\bt\r\n",
      "              shuffle, but group identical keys.  See shuf(1)\r\n",
      "\r\n",
      "       -\b--\b-r\bra\ban\bnd\bdo\bom\bm-\b-s\bso\bou\bur\brc\bce\be=_\bF_\bI_\bL_\bE\r\n",
      "              get random bytes from FILE\r\n",
      "\r\n",
      "       -\b-r\br, -\b--\b-r\bre\bev\bve\ber\brs\bse\be\r\n",
      "              reverse the result of comparisons\r\n",
      "\r\n",
      "       -\b--\b-s\bso\bor\brt\bt=_\bW_\bO_\bR_\bD\r\n",
      "              sort according to WORD: general-numeric  -\b-g\bg,  human-numeric  -\b-h\bh,\r\n",
      "              month -\b-M\bM, numeric -\b-n\bn, random -\b-R\bR, version -\b-V\bV\r\n",
      "\r\n",
      "       -\b-V\bV, -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn-\b-s\bso\bor\brt\bt\r\n",
      "              natural sort of (version) numbers within text\r\n",
      "\r\n",
      "       Other options:\r\n",
      "\r\n",
      "       -\b--\b-b\bba\bat\btc\bch\bh-\b-s\bsi\biz\bze\be=_\bN_\bM_\bE_\bR_\bG_\bE\r\n",
      "              merge at most NMERGE inputs at once; for more use temp files\r\n",
      "\r\n",
      "       -\b-c\bc, -\b--\b-c\bch\bhe\bec\bck\bk, -\b--\b-c\bch\bhe\bec\bck\bk=_\bd_\bi_\ba_\bg_\bn_\bo_\bs_\be_\b-_\bf_\bi_\br_\bs_\bt\r\n",
      "              check for sorted input; do not sort\r\n",
      "\r\n",
      "       -\b-C\bC, -\b--\b-c\bch\bhe\bec\bck\bk=_\bq_\bu_\bi_\be_\bt, -\b--\b-c\bch\bhe\bec\bck\bk=_\bs_\bi_\bl_\be_\bn_\bt\r\n",
      "              like -\b-c\bc, but do not report first bad line\r\n",
      "\r\n",
      "       -\b--\b-c\bco\bom\bmp\bpr\bre\bes\bss\bs-\b-p\bpr\bro\bog\bgr\bra\bam\bm=_\bP_\bR_\bO_\bG\r\n",
      "              compress temporaries with PROG; decompress them with PROG -\b-d\bd\r\n",
      "\r\n",
      "       -\b--\b-d\bde\beb\bbu\bug\bg\r\n",
      "              annotate the part of the line used to sort, and warn about ques-\r\n",
      "              tionable usage to stderr\r\n",
      "\r\n",
      "       -\b--\b-f\bfi\bil\ble\bes\bs0\b0-\b-f\bfr\bro\bom\bm=_\bF\r\n",
      "              read input from the files specified by NUL-terminated  names  in\r\n",
      "              file F; If F is - then read names from standard input\r\n",
      "\r\n",
      "       -\b-k\bk, -\b--\b-k\bke\bey\by=_\bK_\bE_\bY_\bD_\bE_\bF\r\n",
      "              sort via a key; KEYDEF gives location and type\r\n",
      "\r\n",
      "       -\b-m\bm, -\b--\b-m\bme\ber\brg\bge\be\r\n",
      "              merge already sorted files; do not sort\r\n",
      "\r\n",
      "       -\b-o\bo, -\b--\b-o\bou\but\btp\bpu\but\bt=_\bF_\bI_\bL_\bE\r\n",
      "              write result to FILE instead of standard output\r\n",
      "\r\n",
      "       -\b-s\bs, -\b--\b-s\bst\bta\bab\bbl\ble\be\r\n",
      "              stabilize sort by disabling last-resort comparison\r\n",
      "\r\n",
      "       -\b-S\bS, -\b--\b-b\bbu\buf\bff\bfe\ber\br-\b-s\bsi\biz\bze\be=_\bS_\bI_\bZ_\bE\r\n",
      "              use SIZE for main memory buffer\r\n",
      "\r\n",
      "       -\b-t\bt, -\b--\b-f\bfi\bie\bel\bld\bd-\b-s\bse\bep\bpa\bar\bra\bat\bto\bor\br=_\bS_\bE_\bP\r\n",
      "              use SEP instead of non-blank to blank transition\r\n",
      "\r\n",
      "       -\b-T\bT, -\b--\b-t\bte\bem\bmp\bpo\bor\bra\bar\bry\by-\b-d\bdi\bir\bre\bec\bct\bto\bor\bry\by=_\bD_\bI_\bR\r\n",
      "              use  DIR  for temporaries, not $TMPDIR or _\b/_\bt_\bm_\bp; multiple options\r\n",
      "              specify multiple directories\r\n",
      "\r\n",
      "       -\b--\b-p\bpa\bar\bra\bal\bll\ble\bel\bl=_\bN\r\n",
      "              change the number of sorts run concurrently to N\r\n",
      "\r\n",
      "       -\b-u\bu, -\b--\b-u\bun\bni\biq\bqu\bue\be\r\n",
      "              with -\b-c\bc, check for strict ordering; without -\b-c\bc, output only  the\r\n",
      "              first of an equal run\r\n",
      "\r\n",
      "       -\b-z\bz, -\b--\b-z\bze\ber\bro\bo-\b-t\bte\ber\brm\bmi\bin\bna\bat\bte\bed\bd\r\n",
      "              line delimiter is NUL, not newline\r\n",
      "\r\n",
      "       -\b--\b-h\bhe\bel\blp\bp display this help and exit\r\n",
      "\r\n",
      "       -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn\r\n",
      "              output version information and exit\r\n",
      "\r\n",
      "       KEYDEF  is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where\r\n",
      "       F is a field number and C a character position in the field;  both  are\r\n",
      "       origin 1, and the stop position defaults to the line's end.  If neither\r\n",
      "       -\b-t\bt nor -\b-b\bb is in effect, characters in a  field  are  counted  from  the\r\n",
      "       beginning of the preceding whitespace.  OPTS is one or more single-let-\r\n",
      "       ter ordering options  [bdfgiMhnRrV],  which  override  global  ordering\r\n",
      "       options  for  that key.  If no key is given, use the entire line as the\r\n",
      "       key.  Use -\b--\b-d\bde\beb\bbu\bug\bg to diagnose incorrect key usage.\r\n",
      "\r\n",
      "       SIZE may be followed by the following multiplicative suffixes: % 1%  of\r\n",
      "       memory, b 1, K 1024 (default), and so on for M, G, T, P, E, Z, Y.\r\n",
      "\r\n",
      "       ***  WARNING  ***  The locale specified by the environment affects sort\r\n",
      "       order.  Set LC_ALL=C to get the traditional sort order that uses native\r\n",
      "       byte values.\r\n",
      "\r\n",
      "A\bAU\bUT\bTH\bHO\bOR\bR\r\n",
      "       Written by Mike Haertel and Paul Eggert.\r\n",
      "\r\n",
      "R\bRE\bEP\bPO\bOR\bRT\bTI\bIN\bNG\bG B\bBU\bUG\bGS\bS\r\n",
      "       GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\r\n",
      "       Report sort translation bugs to <http://translationproject.org/team/>\r\n",
      "\r\n",
      "C\bCO\bOP\bPY\bYR\bRI\bIG\bGH\bHT\bT\r\n",
      "       Copyright  (C) 2016 Free Software Foundation, Inc.  License GPLv3+: GNU\r\n",
      "       GPL version 3 or later <http://gnu.org/licenses/gpl.html>.\r\n",
      "       This is free software: you are free  to  change  and  redistribute  it.\r\n",
      "       There is NO WARRANTY, to the extent permitted by law.\r\n",
      "\r\n",
      "S\bSE\bEE\bE A\bAL\bLS\bSO\bO\r\n",
      "       shuf(1), uniq(1)\r\n",
      "\r\n",
      "       Full documentation at: <http://www.gnu.org/software/coreutils/sort>\r\n",
      "       or available locally via: info '(coreutils) sort invocation'\r\n",
      "\r\n",
      "GNU coreutils 8.25               February 2017                         SORT(1)\r\n"
     ]
    }
   ],
   "source": [
    "!man sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Parallel Word Count (part 2)\n",
    "Congratulations, you've just implemented a Map-Reduce algorithm! From here on out, we'll refer to the two python scripts you passed to `pWordCount_v1.sh` as '_mapper_' and '_reducer_'. The bash script itself served as our '_framework_' -- it split up the original input, then ___mapped___ our word counting script on to each chunk, then ___aggregated (a.k.a. reduced)___ the resulting files by piping them into our collation script.  Unfortunately, as you may have realized already, there is a major scalability concern with this particular implementation. __In this question you'll fix our implementation of parallel word count so that it will be scalable.__\n",
    "\n",
    "__HINT:__ MapReduce uses the Merge-Sort algorithm under the hood. Linux `sort` command has a merge option which you can use to simiulate the MapReduce framework. Use the `man sort` command to find more information on this option. \n",
    "\n",
    "### Q7 Tasks:\n",
    "\n",
    "* __a) short response:__ What is the potential scalability problem with the provided implementation of `aggregateCounts_v1.py`? Explain why this supposedly 'parallelized' Word Count wouldn't work on a really large input corpus.  [`HINT:` _see the intro to Q6_]\n",
    "\n",
    "\n",
    "* __b) code:__ Fortunately, a 'strategic sort' can solve this problem. Read the instructions at the top of `pWordCount_v2.sh` carefully then make your changes that alphabetically sort the output from the mappers (`countfiles`) before piping them into the reducer script.\n",
    "\n",
    "\n",
    "* __c) code:__ Write the main part of `aggregateCounts_v2.py` so that it takes advantage of the sorted input to add duplicate counts without storing the whole vocabulary in memory. Refer to the file docstring for more detailed instructions. Confirm that your implementation works by running it on both the test and true data files.\n",
    "\n",
    "\n",
    "* __d) short response:__ If you are paying close attention, this rewritten reducer sets us up for a truly scalable solution, but doesn't get us all the way there. In particular, while we chunked our data so it can be processed by multiple mappers, we're still streaming the entire dataset through one reduce script. If the vocabulary is too large to fit on a single computer, we might split the word counts in half after sorting them, then perform the reducing on two separate machines. Explain what could go wrong with this approach. (For now, ignore the question of how we'd sort a dataset that is too large to fit on a single machine and just focus on what might be wrong about the result of this split-in-half reducing).\n",
    "\n",
    "\n",
    "* __e) short response:__ Can you come up with a different way of splitting up the data that would allow us to perform the reducing on separate machines without needing any postprocessing? This is a theoretical question -- don't worry if you don't know how to implement your idea in a bash script, just describe how you'd want to split the sorted counts into different files to be reduced separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "\n",
    "> __a)__ aggregateCounts_v1.py utilizes defaultdict dictionary for counting the words. If the vocabulary is too large, the dictionary which will be stored in-memory may cause the code to crash.\n",
    "\n",
    "> __b-c)__ _complete the coding portions of this question before answering d and e._\n",
    "\n",
    "> __d)__ If we split the sorted wordcounts data in half and perform `reducer` operation on 2 separate nodes, then there could be a chance where one word is split into 2 nodes for reducing($3$ `alice` to machine1 & 1 `alice` to machine$2$); and thus creating a duplicate of the word after reducing on 2 machines.\n",
    "\n",
    "> __e)__ We need to split the data so that no word should go to 2 machines. Instead of looking for exact half split, we should split the data where occurances of one word is completely done around the half split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pWordCount_v2.sh\n",
    "!chmod a+x aggregateCounts_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\r\n",
      "file\t3\r\n",
      "for\t1\r\n",
      "has\t1\r\n",
      "is\t2\r\n",
      "lines\t1\r\n",
      "small\t3\r\n",
      "test\t3\r\n",
      "this\t3\r\n"
     ]
    }
   ],
   "source": [
    "# part c - test your code on the test file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice_test.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - run your code on the Alice file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# part c - confirm that your 'alice' count is correct (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Scalability Considerations\n",
    "In your reading for Week 1's live session, [Chapter1, section 2](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf) of _Data Intensive Text Processing with MapReduce_, Lin and Dyer discuss a number of \"Big Ideas\" that underlie large scale processing: __scale \"out,\" not \"up\"; assume failures are common; move processing to the data; process data sequentially and avoid random access; hide system-level details from the application developer; and seamless scalability.__ Part of our work this semester will be to interact with these ideas in a practical way, not just a conceptual one.\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ What do Lin and Dyer consider the two features of an \"ideal algorithm\" from a scalability perspective?\n",
    "\n",
    "\n",
    "* __b) short response:__ The mapper script below (created on the fly using a little Jupyter magic) will help us illustrate the concept of scalability. Run the provided code which passes this mapper script to our parallel computation 'framework' and runs the 'analysis' on the _Alice_ text file. Note that we've omitted a reducer for simplicity. What do you observe about the time it takes to run this \"algorithm\" when we use 1, 2 and 4 partitions? Does it meet Lin and Dyer's criteria?\n",
    "\n",
    "\n",
    "* __c) short response:__ Let's try something similar with your Word Count analysis. Run the provided code to time your implementation with 1, 2, 4 and 8 partitions. What do you observe about the runtimes? Does this match your expectation? Speculate about why we might be seeing these times. What conclusions should we draw about the scalability of our implementation? [`HINT:` _consider the limitations of both your machine and our implementation... there are some competing forces at work, what are they?_]\n",
    "\n",
    "\n",
    "* __d) OPTIONAL:__ Which components of your Map-Reduce algorithm are affected by a change in the number of partitions? Does increasing the number of partitions increase or decrease the total time spent on each of these portions of the task? What tradeoff does this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "\n",
    "> __a)__ According to Lin and Dyer's consideration, from scalability perspective, an `ideal algorithm` should have 2 characteristics: \n",
    "1. If we increase the data by twice the normal size, the algorithm should take no more than twice the amount to execute the code provided everything is kept the same.\n",
    "2. If we increase the computation(clusters) power by twice, the algorithm should execute the code in no more then half the time provided everything else is kept the same.\n",
    "\n",
    "> __b)__ From below execution for $8.b$, we see that when we increase the number of resources(partitions) from 1 to 2 to 4, the corresponding time taken to execute the code is **nearly halved**. This does not meet Lin and Dyer's criteria for an `ideal algorithm` which asserts the time executed should be **at most half the time** if we increase the computation by a factor of $2$\n",
    "\n",
    "> __c)__ From below execution for $8.c$, as the number of resources(partitions) increase, the ime taken for the entire execution of code is increased. This **does not match** our expectation and is completely against Lin and Dye's criteria for `ideal algorithm`. As its performance matches our expectation for mapper code and does not match for mapper & reducer, we can speculate that the time delays are caused by **reducer code**. So, based on our obseravtion for mapper & reducer code, we can conclude that `pWordCount_v2.sh` is not scalable implementation in terms of the resources(partitions)\n",
    "\n",
    "> __d)__ Mapper and Reducer components of `pWordCount_v2.sh` are affected by the increase/decrease of the number of resources. Increase in the number of resources **decrease the mapper** execution time and **increase the reducer** execution time. Increase in the reducer execution time is due to **sort merging** the large number of cunks of data as the partitions increased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to create the mapper referenced in `part b`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'demo': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This mapper reads from STDIN and waits 0.001 seconds per line.\n",
    "Its only purpose is to demonstrate one of the scalability ideas.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "for line in sys.stdin:\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the mapper is executable\n",
    "!chmod a+x demo/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the next three cells to apply our demo mapper with 1, 2 and 4 partitions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 4.79 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.75 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.78 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to repeat this process with your word count algorithm.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 790 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 800 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 875 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.01 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 8 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.28 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 16 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.83 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 32 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Embarrassingly Parallel\n",
    "`\"If any one of them can explain it,\" said Alice, (she had grown so large in the last few minutes that she wasn’t a bit afraid of interrupting him,) \"I’ll give him sixpence. I don’t believe there’s an atom of meaning in it.\"`\n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 12  </div>\n",
    "\n",
    "### Q9 Tasks:\n",
    "\n",
    "* __a) short response:__ Describe what we mean by 'Embarrassingly Parallel' in terms of word counting. Does this term describe a 'task'? an 'implementation of a task'? \n",
    "\n",
    "* __b) OPTIONAL__: Define this concept in terms of 'associative' and 'commutative' operations. [`HINT:` _Refer to Week 2 Async, particularly section 2.4 and the assigned_ [reading from EECS](https://patterns.eecs.berkeley.edu/?page_id=37) _about Map-Reduce Patterns_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "\n",
    "> __a)__ In word counting problem, parallel processes can perform the counting of the words parllelly and reduce at the end and requires no furthur communication in between the processes. This is an example of **`perfectly parallel`** solution as the instructions are given at the beginning of the task and results are collected at the end with no process dependency. Embarrassingly Parallel solution in terms of word counting **is an implementation of a task**\n",
    "\n",
    "> __b)__ Type your (OPTIONAL) answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW1! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "951px",
    "left": "0px",
    "right": "1561px",
    "top": "106px",
    "width": "600px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
